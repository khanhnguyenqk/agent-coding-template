{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "\n",
    "api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"Anthropic API key not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "\n",
    "system_prompt_env = Environment(loader=FileSystemLoader(\"/Users/khannguyen/projects/agent-coding-template/src/agent_coding_template/prompts\"))\n",
    "usecase_prompt_env = Environment(loader=FileSystemLoader(\"/Users/khannguyen/projects/agent-coding-template/use-case-tests/nvidia-eval-adaptors\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content=[{'text': \"I'll help you understand what this code repository is about. Let me check the README file first to get an overview.\", 'type': 'text'}, {'id': 'toolu_01CUb6jA6LZXmf2cbQJzHPcT', 'input': {'directory': '/Users/khannguyen/projects/evaluations/ToolTalk'}, 'name': 'ReadmeReaderTool', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_01395U2oboJeCunX8TGYPc4i', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 841, 'output_tokens': 97}}, id='run-6111dd98-2c95-4671-bdd7-fac4bbb21f3c-0', tool_calls=[{'name': 'ReadmeReaderTool', 'args': {'directory': '/Users/khannguyen/projects/evaluations/ToolTalk'}, 'id': 'toolu_01CUb6jA6LZXmf2cbQJzHPcT', 'type': 'tool_call'}], usage_metadata={'input_tokens': 841, 'output_tokens': 97, 'total_tokens': 938, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='# :wrench: ToolTalk :speech_balloon:\\n\\n<p align=\"center\">\\n   <a href=\"https://arxiv.org/abs/2311.10775\" target=\"_blank\">:page_facing_up: Paper</a> | <a href=\"mailto:tooltalk@service.microsoft.com\" target=\"_blank\">:mailbox: Contact</a>\\n</p>\\n\\nIntroducing ToolTalk a benchmark for evaluating Tool LLMs in a conversational setting.\\n\\n## Details\\n\\n<div align=\"center\">\\n\\t<img src=\"images/Evaluation.png\" width=90% />\\n</div>\\n\\nToolTalk is designed to evaluate tool-augmented LLMs as a chatbot,\\nan increasingly popular paradigm for everyday users to harness the power of LLMs.\\nToolTalk contains a handcrafted dataset of 28 easy conversations and 50 hard conversations.\\nWe annotate these conversations to contain a ground truth usage of 28 unique tools belonging to 7 themed \"plugins\".\\n\\nEvaluation consists of prompting an LLM to predict the correct sequence of tools after every user utterance in a conversation.\\nThus, evaluating on a single conversation requires an LLM to correctly predict multiple sub-tasks.\\nPredictions are compared against the ground truth to determine success for a single conversation.\\n\\nWe evaluate two chatbots on ToolTalk powered by gpt-3.5-turbo-0613 and gpt-4-0613 implemented by using the chat completions API from OpenAI.\\n\\n| Model   | ToolTalk | Success rate | Precision | Recall | Incorrect Action Rate |\\n|---------|----------|--------------|-----------|--------|-----------------------|\\n| GPT-3.5 | Easy     | 85.7%        | 42.4%     | 89.3%  | 5.0%                  |\\n| GPT-4   | Easy     | 92.8%        | 69.2%     | 96.4%  | 3.8%                  |\\n| GPT-3.5 | Hard     | 26.0%        | 54.6%     | 69.7%  | 23.9%                 |\\n| GPT-4   | Hard     | 50.0%        | 74.9%     | 79.0%  | 25.1%                 |\\n\\n## Setup\\n\\nToolTalk can be setup using the following commands. Install local package with dev dependencies to enable unit tests.\\n\\n```bash\\npip install -r requirements.txt\\npip install -e \".[dev]\"\\n```\\n\\nTo verify that the installation was successful, run the unit tests.\\n\\n```bash\\npytest tests\\n```\\n\\n## Reproducing the results\\n\\nThe results on GPT-3.5-turbo and GPT-4 can be reproduced using the following commands. This requires having access to \\nOpenAI\\'s API. The results will be saved in the `results` folder. The script caches intermediary results, so it can be \\nre-run if it is interrupted for any reason.\\n\\n```bash\\nexport OPENAI_API_KEY=<your key>\\nbash evaluate_gpt35turbo.sh\\nbash evaluate_gpt4.sh\\n```\\n\\nYour results should look something like the number above, there will be some variance due to both models having non-deterministic results.\\n\\n## Generating scenarios\\n\\nTo generate new scenarios, you can use the following command.\\n\\n```bash\\npython -m tooltalk.generation.scenario_generator --prompt src/prompts/scenario_template.md --output_dir output/scenarios\\n```\\n\\n## Evaluating on new models\\n\\nThe easiest way to evaluate on new models would be to create a new `Predictor` class that inherits from `tooltalk.evaluation.tool_executor.BaseAPIPredictor`.\\nFor an example of how to do this, see `tooltalk.evaluation.tool_executor.GPT3Predictor` and `tooltalk.evaluation.evaluate_openai.OpenAIPredictor`.\\n\\n## Citing\\n\\n```\\n@article{farn2023tooltalk,\\n  title={ToolTalk: Evaluating Tool-Usage in a Conversation Setting},\\n  author={Nicholas Farn and Richard Shin},\\n  year={2023},\\n  journal={arXiv preprint arXiv:2311.10775},\\n}\\n```\\n\\n## Contributing\\n\\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\\n\\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\\nprovided by the bot. You will only need to do this once across all repos using our CLA.\\n\\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\\n\\n## Trademarks\\n\\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \\ntrademarks or logos is subject to and must follow \\n[Microsoft\\'s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\\nAny use of third-party trademarks or logos are subject to those third-party\\'s policies.\\n', name='ReadmeReaderTool', id='6e38e382-e005-4c95-bd64-d0161a19d0cc', tool_call_id='toolu_01CUb6jA6LZXmf2cbQJzHPcT')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='Let me help explain what this repository is about:\\n\\nToolTalk is a benchmark created to evaluate Tool-augmented Large Language Models (LLMs) in a conversational setting. Here are the key points about the repository:\\n\\n1. Purpose:\\n- It\\'s designed to evaluate how well LLMs can use tools while acting as a chatbot\\n- The benchmark tests the LLM\\'s ability to predict the correct sequence of tools in response to user conversations\\n\\n2. Dataset:\\n- Contains 28 easy conversations and 50 hard conversations\\n- Includes annotations for 28 unique tools spread across 7 themed \"plugins\"\\n- Each conversation requires multiple sub-tasks to be completed correctly\\n\\n3. Evaluation Results:\\n- The benchmark has been tested with GPT-3.5-turbo and GPT-4\\n- GPT-4 performs better overall:\\n  - On easy conversations: 92.8% success rate\\n  - On hard conversations: 50.0% success rate\\n- GPT-3.5 performance:\\n  - On easy conversations: 85.7% success rate\\n  - On hard conversations: 26.0% success rate\\n\\n4. Implementation:\\n- The repository includes code to:\\n  - Set up and run the benchmark\\n  - Generate new scenarios\\n  - Evaluate different models\\n  - Reproduce results with GPT-3.5-turbo and GPT-4\\n\\n5. Usage:\\n- Can be installed using pip\\n- Requires OpenAI API access for reproducing results\\n- Supports creating custom evaluations for new models\\n- Includes unit tests to verify installation\\n\\nThis is an open-source project from Microsoft, welcoming contributions and following Microsoft\\'s Open Source Code of Conduct. It represents a significant contribution to understanding and evaluating how well LLMs can use tools in conversational contexts.', additional_kwargs={}, response_metadata={'id': 'msg_01Bvq4cbSQdMNbLnkZDV36xC', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 2305, 'output_tokens': 409}}, id='run-299f3c8a-ffe4-45ca-b171-eba80761a679-0', usage_metadata={'input_tokens': 2305, 'output_tokens': 409, 'total_tokens': 2714, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Import relevant functionality\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "from agent_coding_template.tools import *\n",
    "\n",
    "# Create the agent\n",
    "memory = MemorySaver()\n",
    "model = ChatAnthropic(model_name=\"claude-3-5-sonnet-20241022\")\n",
    "tools = [DirectoryMapperTool(), FileReaderTool(), ReadmeReaderTool()]\n",
    "agent_executor = create_react_agent(model, tools, checkpointer=memory)\n",
    "\n",
    "# Use the agent\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=template.render(repo_dir=\"/Users/khannguyen/projects/evaluations/ToolTalk\"))]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content=[{'text': 'Let me check the repository structure to find the relevant implementation files that you would need to adapt.', 'type': 'text'}, {'id': 'toolu_01Ls3Miae7Zj3y1FK4vBAM3g', 'input': {'directory': '/Users/khannguyen/projects/evaluations/ToolTalk'}, 'name': 'DirectoryMapperTool', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_01LpEU2A3h3EoKMFWTuVGAPM', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 2734, 'output_tokens': 92}}, id='run-f4cb54fa-6298-4e38-a880-4ff6164827c3-0', tool_calls=[{'name': 'DirectoryMapperTool', 'args': {'directory': '/Users/khannguyen/projects/evaluations/ToolTalk'}, 'id': 'toolu_01Ls3Miae7Zj3y1FK4vBAM3g', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2734, 'output_tokens': 92, 'total_tokens': 2826, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='ToolTalk/\\n├── CODE_OF_CONDUCT.md\\n├── CONTRIBUTING.md\\n├── LICENSE\\n├── NOTICE\\n├── README.md\\n├── SECURITY.md\\n├── SUPPORT.md\\n├── data/\\n│   ├── databases/\\n│   │   ├── Account.json\\n│   │   ├── Alarm.json\\n│   │   ├── Calendar.json\\n│   │   ├── Email.json\\n│   │   ├── HistoricWeather.json\\n│   │   ├── Message.json\\n│   │   ├── Reminder.json\\n│   │   └── Weather.json\\n│   ├── easy/\\n│   │   ├── AddAlarm-easy.json\\n│   │   ├── AddReminder-easy.json\\n│   │   ├── ChangePassword-easy.json\\n│   │   ├── CompleteReminder-easy.json\\n│   │   ├── CreateEvent-easy.json\\n│   │   ├── CurrentWeather-easy.json\\n│   │   ├── DeleteAccount-easy.json\\n│   │   ├── DeleteAlarm-easy.json\\n│   │   ├── DeleteEvent-easy.json\\n│   │   ├── DeleteReminder-easy.json\\n│   │   ├── FindAlarms-easy.json\\n│   │   ├── ForecastWeather-easy.json\\n│   │   ├── GetAccountInformation-easy.json\\n│   │   ├── GetReminders-easy.json\\n│   │   ├── HistoricWeather-easy.json\\n│   │   ├── LogoutUser-easy.json\\n│   │   ├── ModifyEvent-easy.json\\n│   │   ├── QueryCalendar-easy.json\\n│   │   ├── QueryUser-easy.json\\n│   │   ├── RegisterUser-easy.json\\n│   │   ├── ResetPassword-easy.json\\n│   │   ├── SearchInbox-easy.json\\n│   │   ├── SearchMessages-easy.json\\n│   │   ├── SendEmail-easy.json\\n│   │   ├── SendMessage-easy.json\\n│   │   ├── SendVerificationCode-easy.json\\n│   │   ├── UpdateAccountInformation-easy.json\\n│   │   └── UserLogin-easy.json\\n│   └── tooltalk/\\n│       ├── AccountTools-Alarm-Calendar-AddAlarm-0.json\\n│       ├── AccountTools-Alarm-Email-GetAccountInformati-0.json\\n│       ├── AccountTools-Alarm-Messages-FindAlarm-0.json\\n│       ├── AccountTools-Alarm-Weather-DeleteAccount-2.json\\n│       ├── AccountTools-Calendar-Weather-RegisterUser-0.json\\n│       ├── AccountTools-Email-Reminder-ChangePassword-1.json\\n│       ├── Alarm-Calendar-Email-DeleteAlarm-1.json\\n│       ├── Alarm-Calendar-Messages-AddAlarm-1.json\\n│       ├── Alarm-Messages-Reminder-GetReminder-2.json\\n│       ├── Alarm-Reminder-Weather-AddReminder-1.json\\n│       ├── Alarm-Reminder-Weather-DeleteAlarm-2.json\\n│       ├── Alarm-Reminder-Weather-FindAlarm-2.json\\n│       ├── Calendar-Email-Reminder-GetReminder-1.json\\n│       ├── Calendar-Email-Reminder-SendEmail-2.json\\n│       ├── Calendar-Messages-Reminder-AddReminder-1.json\\n│       ├── Calendar-Messages-Reminder-CreateEvent-1.json\\n│       ├── Calendar-Messages-Reminder-ModifyEvent-0.json\\n│       ├── Calendar-Messages-Reminder-QueryCalendar-2.json\\n│       ├── Calendar-Messages-Weather-DeleteEvent-0.json\\n│       ├── Calendar-Messages-Weather-SendMessage-0.json\\n│       ├── Calendar-Messages-Weather-SendMessage-1.json\\n│       ├── Calendar-Reminder-Weather-CompleteReminder-1.json\\n│       ├── Calendar-Reminder-Weather-CreateEvent-1.json\\n│       ├── Calendar-Reminder-Weather-DeleteEvent-0.json\\n│       ├── Calendar-Reminder-Weather-DeleteReminder-1.json\\n│       ├── Calendar-Reminder-Weather-ModifyEvent-0.json\\n│       ├── Calendar-Reminder-Weather-ModifyEvent-1.json\\n│       ├── Calendar-Reminder-Weather-QueryCalendar-0.json\\n│       ├── Email-Messages-Reminder-DeleteReminder-1.json\\n│       ├── Email-Messages-Reminder-SearchMessage-1.json\\n│       ├── Email-Messages-Reminder-SendEmail-2.json\\n│       ├── Email-Messages-Reminder-SendMessage-2.json\\n│       ├── Email-Messages-Weather-SendMessage-2.json\\n│       ├── Email-Reminder-Weather-CompleteReminder-2.json\\n│       ├── Email-Reminder-Weather-DeleteReminder-0.json\\n│       ├── Email-Reminder-Weather-GetReminder-0.json\\n│       ├── Email-Reminder-Weather-SearchInbox-1.json\\n│       ├── Email-Reminder-Weather-SendEmail-2.json\\n│       ├── Messages-Reminder-Weather-AddReminder-1.json\\n│       ├── Messages-Reminder-Weather-CompleteReminder-1.json\\n│       ├── Messages-Reminder-Weather-ForecastWeather-1.json\\n│       ├── Messages-Reminder-Weather-GetReminder-0.json\\n│       ├── Messages-Reminder-Weather-SearchMessage-2.json\\n│       ├── Messages-Reminder-Weather-SendMessage-2.json\\n│       ├── golden_conversation_1.json\\n│       ├── golden_conversation_2.json\\n│       ├── golden_conversation_3.json\\n│       ├── golden_conversation_4.json\\n│       ├── golden_conversation_5.json\\n│       └── golden_conversation_6.json\\n├── images/\\n│   └── Evaluation.png\\n├── output/\\n│   └── nvdev/\\n│       └── meta/\\n│           ├── llama-3.1-70b-instruct/\\n│           │   ├── AddAlarm-easy.json\\n│           │   ├── AddReminder-easy.json\\n│           │   ├── ChangePassword-easy.json\\n│           │   ├── CompleteReminder-easy.json\\n│           │   ├── CreateEvent-easy.json\\n│           │   ├── CurrentWeather-easy.json\\n│           │   ├── DeleteAccount-easy.json\\n│           │   ├── DeleteAlarm-easy.json\\n│           │   ├── DeleteEvent-easy.json\\n│           │   ├── DeleteReminder-easy.json\\n│           │   ├── FindAlarms-easy.json\\n│           │   ├── ForecastWeather-easy.json\\n│           │   ├── GetAccountInformation-easy.json\\n│           │   ├── GetReminders-easy.json\\n│           │   ├── HistoricWeather-easy.json\\n│           │   ├── LogoutUser-easy.json\\n│           │   ├── ModifyEvent-easy.json\\n│           │   ├── QueryCalendar-easy.json\\n│           │   ├── QueryUser-easy.json\\n│           │   ├── RegisterUser-easy.json\\n│           │   ├── ResetPassword-easy.json\\n│           │   ├── SearchInbox-easy.json\\n│           │   ├── SearchMessages-easy.json\\n│           │   ├── SendEmail-easy.json\\n│           │   ├── SendMessage-easy.json\\n│           │   ├── SendVerificationCode-easy.json\\n│           │   ├── UpdateAccountInformation-easy.json\\n│           │   ├── UserLogin-easy.json\\n│           │   └── metrics.json\\n│           └── llama-3.1-70b-instruct2/\\n├── requirements.txt\\n├── scripts/\\n│   ├── evaluate_gpt35turbo.sh\\n│   └── evaluate_gpt4.sh\\n├── setup.cfg\\n├── setup.py\\n├── src/\\n│   ├── __init__.py\\n│   ├── prompts/\\n│   │   └── scenario_template.md\\n│   ├── scripts/\\n│   │   ├── fix_apis_used.py\\n│   │   └── refactor_tool_talk.py\\n│   ├── tooltalk/\\n│   │   ├── __init__.py\\n│   │   ├── __pycache__/\\n│   │   │   └── __init__.cpython-312.pyc\\n│   │   ├── apis/\\n│   │   │   ├── __init__.py\\n│   │   │   ├── __pycache__/\\n│   │   │   │   ├── __init__.cpython-312.pyc\\n│   │   │   │   ├── account.cpython-312.pyc\\n│   │   │   │   ├── alarm.cpython-312.pyc\\n│   │   │   │   ├── api.cpython-312.pyc\\n│   │   │   │   ├── calendar.cpython-312.pyc\\n│   │   │   │   ├── email.cpython-312.pyc\\n│   │   │   │   ├── exceptions.cpython-312.pyc\\n│   │   │   │   ├── message.cpython-312.pyc\\n│   │   │   │   ├── reminder.cpython-312.pyc\\n│   │   │   │   ├── utils.cpython-312.pyc\\n│   │   │   │   └── weather.cpython-312.pyc\\n│   │   │   ├── account.py\\n│   │   │   ├── alarm.py\\n│   │   │   ├── api.py\\n│   │   │   ├── calendar.py\\n│   │   │   ├── email.py\\n│   │   │   ├── exceptions.py\\n│   │   │   ├── message.py\\n│   │   │   ├── reminder.py\\n│   │   │   ├── utils.py\\n│   │   │   └── weather.py\\n│   │   ├── evaluation/\\n│   │   │   ├── __init__.py\\n│   │   │   ├── __pycache__/\\n│   │   │   │   ├── __init__.cpython-312.pyc\\n│   │   │   │   └── tool_executor.cpython-312.pyc\\n│   │   │   ├── calculate_error_types.py\\n│   │   │   ├── evaluate_nim.py\\n│   │   │   ├── evaluate_openai.py\\n│   │   │   └── tool_executor.py\\n│   │   ├── generation/\\n│   │   │   ├── __init__.py\\n│   │   │   └── scenario_generator.py\\n│   │   ├── llm_clients/\\n│   │   └── utils/\\n│   │       ├── __init__.py\\n│   │       ├── __pycache__/\\n│   │       │   ├── __init__.cpython-312.pyc\\n│   │       │   ├── file_utils.cpython-312.pyc\\n│   │       │   └── openai_utils.cpython-312.pyc\\n│   │       ├── file_utils.py\\n│   │       └── openai_utils.py\\n│   └── tooltalk.egg-info/\\n│       ├── PKG-INFO\\n│       ├── SOURCES.txt\\n│       ├── dependency_links.txt\\n│       ├── requires.txt\\n│       └── top_level.txt\\n└── tests/\\n    ├── __pycache__/\\n    │   ├── test_documentation.cpython-312-pytest-7.4.4.pyc\\n    │   ├── test_documentation.cpython-312-pytest-8.3.4.pyc\\n    │   ├── test_oracle.cpython-312-pytest-7.4.4.pyc\\n    │   └── test_oracle.cpython-312-pytest-8.3.4.pyc\\n    ├── test_documentation.py\\n    └── test_oracle.py', name='DirectoryMapperTool', id='2d2bc4b6-9c5d-4c99-945d-756c22324dcc', tool_call_id='toolu_01Ls3Miae7Zj3y1FK4vBAM3g')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content=[{'text': 'Let me check the implementation of the tool executor to understand how to adapt it.', 'type': 'text'}, {'id': 'toolu_01KU51CYPqL4ksLJ7vkuZ9ZK', 'input': {'file_path': '/Users/khannguyen/projects/evaluations/ToolTalk/src/tooltalk/evaluation/tool_executor.py'}, 'name': 'FileReaderTool', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_01TCUBWJbkRux6uQ2LLMPy2G', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 5988, 'output_tokens': 104}}, id='run-462048b6-e8f7-4976-899c-0ca5e4a5c204-0', tool_calls=[{'name': 'FileReaderTool', 'args': {'file_path': '/Users/khannguyen/projects/evaluations/ToolTalk/src/tooltalk/evaluation/tool_executor.py'}, 'id': 'toolu_01KU51CYPqL4ksLJ7vkuZ9ZK', 'type': 'tool_call'}], usage_metadata={'input_tokens': 5988, 'output_tokens': 104, 'total_tokens': 6092, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='\"\"\"\\nCopyright (c) Microsoft Corporation.\\nLicensed under the MIT license.\\n\"\"\"\\nimport json\\nimport logging\\nimport os\\nfrom typing import List\\nfrom datetime import datetime\\nfrom collections import deque\\nfrom abc import ABC, abstractmethod\\n\\nfrom tooltalk.apis import ALL_APIS\\nfrom tooltalk.apis.account import ACCOUNT_DB_NAME, DeleteAccount, UserLogin, LogoutUser, RegisterUser\\nfrom tooltalk.utils.file_utils import get_names_and_paths\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass ToolExecutor:\\n    \"\"\"\\n    Handles execution of tools and maintains state of databases when simulating conversations.\\n    \"\"\"\\n    def __init__(\\n            self,\\n            init_database_dir: str = None,\\n            ignore_list: List[str] = None,\\n            account_database: str = ACCOUNT_DB_NAME,\\n    ) -> None:\\n        self.databases = dict()\\n        self.database_files = dict()\\n        self.account_database = account_database\\n        self.ignore_list = ignore_list if ignore_list is not None else list()\\n        self.session_token = None\\n\\n        for file_name, file_path in get_names_and_paths(init_database_dir):\\n            database_name, ext = os.path.splitext(file_name)\\n            if ext == \".json\":\\n                self.database_files[database_name] = file_path\\n                with open(file_path, \\'r\\', encoding=\\'utf-8\\') as reader:\\n                    self.databases[database_name] = json.load(reader)\\n        if self.account_database not in self.databases:\\n            raise ValueError(f\"Account database {self.account_database} not found\")\\n\\n        self.apis = {api.__name__: api for api in ALL_APIS if api.__name__ not in self.ignore_list}\\n        self.inited_tools = dict()\\n        self.now_timestamp = None\\n\\n    def reset_executor(self):\\n        \"\"\"\\n        Reset all tools and databases to their initial state.\\n        \"\"\"\\n        self.databases = dict()\\n        for database_name, file_path in self.database_files.items():\\n            with open(file_path, \\'r\\', encoding=\\'utf-8\\') as reader:\\n                self.databases[database_name] = json.load(reader)\\n        self.inited_tools = dict()\\n        self.now_timestamp = None\\n        self.session_token = None\\n\\n    def get_init_tool(self, tool_name: str):\\n        if tool_name in self.inited_tools:\\n            return self.inited_tools[tool_name]\\n        cls = self.apis[tool_name]\\n        account_db = self.databases.get(self.account_database)\\n        if cls.database_name is not None:\\n            database = self.databases.get(cls.database_name)\\n            tool = cls(\\n                account_database=account_db,\\n                now_timestamp=self.now_timestamp,\\n                api_database=database,\\n            )\\n        else:\\n            tool = cls(\\n                account_database=account_db,\\n                now_timestamp=self.now_timestamp,\\n            )\\n\\n        self.inited_tools[tool_name] = tool\\n        return tool\\n\\n    def execute_tool(self, api_name: str, parameters: dict):\\n        request = {\\n            \"api_name\": api_name,\\n            \"parameters\": parameters\\n        }\\n        if api_name not in self.apis:\\n            response = {\\n                \"response\": None,\\n                \"exception\": f\"API {api_name} not found\"\\n            }\\n            return request, response\\n\\n        tool = self.get_init_tool(api_name)\\n        if tool.requires_auth:\\n            if self.session_token is None:\\n                response = {\\n                    \"response\": None,\\n                    \"exception\": \"User is not logged in\"\\n                }\\n                return request, response\\n            parameters[\"session_token\"] = self.session_token\\n        if api_name in [UserLogin.__name__, RegisterUser.__name__] and self.session_token is not None:\\n            username = tool.check_session_token(self.session_token)[\"username\"]\\n            response = {\\n                \"response\": None,\\n                \"exception\": f\"Only one user can be logged in at a time. Current user is {username}.\",\\n            }\\n            return request, response\\n\\n        # execute tool\\n        response = tool(**parameters)\\n\\n        # capture session_token and simulate login and logout\\n        if api_name in [UserLogin.__name__, RegisterUser.__name__] and response[\"exception\"] is None:\\n            self.session_token = response[\"response\"][\"session_token\"]\\n        elif api_name in [LogoutUser.__name__, DeleteAccount.__name__] and response[\"exception\"] is None:\\n            self.session_token = None\\n        return request, response\\n\\n    def compare_api_calls(self, prediction: dict, ground_truth: dict) -> bool:\\n        api_name = prediction[\"request\"][\"api_name\"]\\n        if api_name != ground_truth[\"request\"][\"api_name\"]:\\n            return False\\n\\n        # TODO add session_token if ground truth needs it\\n        return self.apis[api_name].check_api_call_correctness(prediction, ground_truth)\\n\\n    def is_action(self, api_name: str) -> bool:\\n        if api_name not in self.apis:\\n            return False\\n        return self.apis[api_name].is_action\\n\\n    def evaluate_predictions(self, conversation_with_predictions: dict) -> dict:\\n        \"\"\"\\n        Compare predictions in a conversation with complete ground truth in conversation returning metrics.\\n        Calculates recall over ground truth, where predictions can only match to function in ground truth once.\\n        Additionally, calculates action precision, number of actions that match ground truth.\\n        Finally, calculates success, which is recall == 1.0 and action precision == 1.0.\\n\\n        Metrics:\\n            predictions: number of predictions\\n            ground_truths: number of ground truths\\n            matches: number of predictions that match ground truth\\n            actions: number of predictions that are actions\\n            valid_actions: number of actions that match ground truth\\n            bad_actions: number of actions that don\\'t match ground truth\\n            precision: matches / predictions\\n            recall: matches / ground_truths\\n            action_precision: valid_actions / actions\\n            bad_action_rate: bad_actions / actions\\n            success: recall == 1.0 and bad_action_rate == 0.0\\n        \"\"\"\\n        predictions = list()\\n        ground_truths = list()\\n        for turn in conversation_with_predictions[\"conversation\"]:\\n            if turn[\"role\"] == \"User\":\\n                continue\\n            if \"predictions\" in turn:\\n                # last prediction will be assistant response\\n                for prediction in turn[\"predictions\"]:\\n                    if prediction[\\'role\\'] == \\'api\\':\\n                        predictions.append(prediction)\\n            if \"apis\" in turn:\\n                ground_truths.extend(turn[\"apis\"])\\n\\n        # remove ground truth as they get matched to predictions\\n        match_count = 0\\n        action_count = 0\\n        valid_action_count = 0\\n        bad_action_count = 0\\n        current_ground_truths = deque(ground_truths)\\n        for prediction in predictions:\\n            is_match = False\\n            new_ground_truths = deque()\\n            while current_ground_truths:\\n                ground_truth = current_ground_truths.popleft()\\n                if self.compare_api_calls(prediction, ground_truth):\\n                    # don\\'t add back in ground truth that matches\\n                    is_match = True\\n                    ground_truth[\"match\"] = True\\n                    break\\n                else:\\n                    new_ground_truths.append(ground_truth)\\n            else:\\n                logger.debug(f\"Failed {json.dumps(prediction, indent=4)}\")\\n\\n            # alter prediction data\\n            is_action = self.is_action(prediction[\"request\"][\"api_name\"])\\n            is_successful = prediction[\"exception\"] is None\\n            is_bad_action = not is_match and is_action and is_successful\\n            prediction[\"match\"] = is_match\\n            prediction[\"bad_action\"] = is_bad_action\\n\\n            # add back in ground truths that don\\'t match\\n            while current_ground_truths:\\n                new_ground_truths.append(current_ground_truths.popleft())\\n            current_ground_truths = new_ground_truths\\n\\n            # update counters\\n            match_count += is_match\\n            action_count += is_action\\n            valid_action_count += is_action and is_match\\n            bad_action_count += is_bad_action\\n\\n        for ground_truth in current_ground_truths:\\n            ground_truth[\"match\"] = False\\n\\n        precision = match_count / len(predictions) if len(predictions) > 0 else 0\\n        recall = match_count / len(ground_truths)\\n        action_precision = valid_action_count / action_count if action_count > 0 else 1\\n        bad_action_rate = bad_action_count / action_count if action_count > 0 else 0\\n        success = recall == 1.0 and bad_action_rate == 0.0\\n        soft_success = recall * (1.0 - bad_action_rate)\\n        metrics = {\\n            \"predictions\": len(predictions),\\n            \"ground_truths\": len(ground_truths),\\n            \"matches\": match_count,\\n            \"actions\": action_count,\\n            \"valid_actions\": valid_action_count,\\n            \"bad_actions\": bad_action_count,\\n            \"precision\": precision,\\n            \"recall\": recall,\\n            \"action_precision\": action_precision,\\n            # number of actions matching ground truth\\n            \"bad_action_rate\": bad_action_rate,\\n            # how often an action is bad aka successful but not matching ground truth\\n            \"success\": success,\\n            \"soft_success\": soft_success,\\n        }\\n        conversation_with_predictions[\"metrics\"] = metrics\\n        return conversation_with_predictions\\n\\n    def init_conversation_state(self, metadata: dict, api_history: list, user_data: dict = None) -> None:\\n        self.reset_executor()\\n        self.now_timestamp = datetime.strptime(metadata[\"timestamp\"], \"%Y-%m-%d %H:%M:%S\")\\n\\n        # setting these should never fail, if it does it\\'s a bug in the dataset\\n        if \"session_token\" in user_data:\\n            username = user_data[\"username\"]\\n            self.session_token = user_data[\"session_token\"]\\n            self.databases[self.account_database][username][\"session_token\"] = user_data[\"session_token\"]\\n        if \"verification_code\" in user_data:\\n            username = user_data[\"username\"]\\n            self.databases[self.account_database][username][\"verification_code\"] = user_data[\"verification_code\"]\\n\\n        for api in api_history:\\n            # this should also never fail, if it does it\\'s a bug in dataset\\n            self.execute_tool(**api[\"request\"])\\n\\n    def run_conversation(self, conversation: dict, predict_func: callable):\\n        \"\"\"\\n        Simulates a conversation, calling prediction function\\n        \"\"\"\\n        metadata = conversation[\"metadata\"]\\n        user_data = conversation.get(\"user\")\\n        ground_truth_history = list()\\n        api_history = list()\\n\\n        for turn in conversation[\"conversation\"]:\\n            if turn[\"role\"] == \"user\":\\n                ground_truth_history.append({\\n                    \"role\": \"user\",\\n                    \"text\": turn[\"text\"]\\n                })\\n                continue\\n\\n            if turn[\"role\"] != \"assistant\":\\n                raise ValueError(f\"turn role must be user or assistant, instead got {turn[\\'role\\']}\")\\n\\n            # other turns should be the assistant and could contain API calls\\n            self.init_conversation_state(metadata, api_history, user_data)\\n            predictions = list()\\n            current_history = ground_truth_history.copy()\\n            while True:\\n                prediction = predict_func(metadata, current_history)\\n                if prediction[\"role\"] == \"assistant\":\\n                    # done with predicting apis\\n                    predictions.append(prediction)\\n                    break\\n                elif prediction[\"role\"] == \"api\":\\n                    # execute api call\\n                    if prediction[\"request\"][\"parameters\"] is None:\\n                        request = prediction[\"request\"]\\n                        response = {\\n                            \"response\": None,\\n                            \"exception\": \"Failed to parse API call\"\\n                        }\\n                    else:\\n                        request, response = self.execute_tool(**prediction[\"request\"])\\n                    prediction_and_response = {\\n                        \"request\": request,\\n                        \"response\": response[\"response\"],\\n                        \"exception\": response[\"exception\"],\\n                        \"metadata\": prediction.get(\"metadata\")\\n                    }\\n                    predictions.append(prediction_and_response)\\n                    prediction_and_response[\"role\"] = \"api\"\\n                    current_history.append(prediction_and_response)\\n                else:\\n                    raise ValueError(f\"prediction role should be api or assistant, instead got {prediction[\\'role\\']}\")\\n\\n            # add predictions to original conversation object\\n            turn[\"predictions\"] = predictions\\n            if \"apis\" in turn:\\n                for api in turn[\"apis\"]:\\n                    api_history.append(api)\\n                    ground_truth_history.append({\\n                        \"role\": \"api\",\\n                        \"request\": api[\"request\"],\\n                        \"response\": api[\"response\"],\\n                        \"exception\": api[\"exception\"]\\n                    })\\n            ground_truth_history.append({\\n                \"role\": \"assistant\",\\n                \"text\": turn[\"text\"]\\n            })\\n\\n        return conversation\\n\\n\\nclass BaseAPIPredictor(ABC):\\n    @abstractmethod\\n    def __init__(self, function_docs: List[dict], *args, **kwargs):\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def predict(self, metadata: dict, conversation_history: dict) -> dict:\\n        raise NotImplementedError\\n\\n    def __call__(self, metadata: dict, conversation_history: dict) -> dict:\\n        \"\"\"Simple wrapper for convenience.\"\"\"\\n        return self.predict(metadata, conversation_history)\\n', name='FileReaderTool', id='04a0695c-8df7-40ab-9538-73cc5a7dd86c', tool_call_id='toolu_01KU51CYPqL4ksLJ7vkuZ9ZK')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='Based on the code examination, here\\'s how you would adapt ToolTalk to your programmatic interface:\\n\\n1. Create a Custom Predictor:\\nThe main integration point is through the `BaseAPIPredictor` class. You would need to create your own predictor class that inherits from this base class:\\n\\n```python\\nclass YourCustomPredictor(BaseAPIPredictor):\\n    def __init__(self, function_docs: List[dict], *args, **kwargs):\\n        # Initialize your model/interface here\\n        self.your_interface = YourInterface(*args, **kwargs)\\n        self.function_docs = function_docs\\n\\n    def predict(self, metadata: dict, conversation_history: dict) -> dict:\\n        # Implement your prediction logic here\\n        # Must return a dict with the following structure:\\n        # For API calls:\\n        # {\\n        #     \"role\": \"api\",\\n        #     \"request\": {\\n        #         \"api_name\": \"name_of_api\",\\n        #         \"parameters\": {param_dict}\\n        #     }\\n        # }\\n        # For assistant responses:\\n        # {\\n        #     \"role\": \"assistant\",\\n        #     \"text\": \"response text\"\\n        # }\\n```\\n\\n2. Integration with the Tool Executor:\\nThe `ToolExecutor` class handles the execution of tools and maintains the state. You would use it like this:\\n\\n```python\\n# Initialize the tool executor\\nexecutor = ToolExecutor(init_database_dir=\"path/to/your/databases\")\\n\\n# Create your predictor\\nyour_predictor = YourCustomPredictor(function_docs=your_function_docs)\\n\\n# Run a conversation\\nresults = executor.run_conversation(\\n    conversation=your_conversation_data,\\n    predict_func=your_predictor\\n)\\n```\\n\\n3. Key Components to Implement:\\n\\na. Function Documentation:\\n- Your predictor needs to understand the available tools\\n- Each tool should be documented in a format that your interface can understand\\n- The original implementation uses JSON format for tool definitions\\n\\nb. Conversation Format:\\nYour conversation data should follow this structure:\\n```python\\n{\\n    \"metadata\": {\\n        \"timestamp\": \"YYYY-MM-DD HH:MM:SS\"\\n    },\\n    \"conversation\": [\\n        {\\n            \"role\": \"user\",\\n            \"text\": \"user message\"\\n        },\\n        {\\n            \"role\": \"assistant\",\\n            \"text\": \"assistant response\",\\n            \"apis\": [  # ground truth API calls\\n                {\\n                    \"request\": {\\n                        \"api_name\": \"ToolName\",\\n                        \"parameters\": {...}\\n                    },\\n                    \"response\": {...},\\n                    \"exception\": null\\n                }\\n            ]\\n        }\\n    ]\\n}\\n```\\n\\n4. Handling Tool Execution:\\n- The `ToolExecutor` will handle the actual execution of tools\\n- It maintains state across the conversation\\n- It tracks things like session tokens and database states\\n- It validates predictions against ground truth\\n\\n5. Evaluation:\\nThe system will automatically:\\n- Track successful API calls\\n- Calculate precision and recall\\n- Identify incorrect actions\\n- Provide metrics on the performance\\n\\nTo adapt this to your interface:\\n\\n1. Implement your custom predictor\\n2. Format your tool definitions to match the expected structure\\n3. Format your conversation data according to the specified format\\n4. Use the `ToolExecutor` to run and evaluate conversations\\n\\nThe main adaptations would happen in:\\n- Your predictor implementation\\n- The format of your tool definitions\\n- How you translate between your interface\\'s format and ToolTalk\\'s expected formats\\n\\nThe core evaluation logic can remain unchanged as long as you conform to the expected input/output formats.\\n\\nWould you like me to show you a more detailed example of any of these components?', additional_kwargs={}, response_metadata={'id': 'msg_01XyZb39s94gdxc9Grxm7hnz', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 9503, 'output_tokens': 882}}, id='run-b96235fe-922e-4471-b5a8-a2778ec3f9fd-0', usage_metadata={'input_tokens': 9503, 'output_tokens': 882, 'total_tokens': 10385, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me how you would adapt this code to a programmatic interface that I defined.\")]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the code examination, here's how you would adapt ToolTalk to your programmatic interface:\n",
      "\n",
      "1. Create a Custom Predictor:\n",
      "The main integration point is through the `BaseAPIPredictor` class. You would need to create your own predictor class that inherits from this base class:\n",
      "\n",
      "```python\n",
      "class YourCustomPredictor(BaseAPIPredictor):\n",
      "    def __init__(self, function_docs: List[dict], *args, **kwargs):\n",
      "        # Initialize your model/interface here\n",
      "        self.your_interface = YourInterface(*args, **kwargs)\n",
      "        self.function_docs = function_docs\n",
      "\n",
      "    def predict(self, metadata: dict, conversation_history: dict) -> dict:\n",
      "        # Implement your prediction logic here\n",
      "        # Must return a dict with the following structure:\n",
      "        # For API calls:\n",
      "        # {\n",
      "        #     \"role\": \"api\",\n",
      "        #     \"request\": {\n",
      "        #         \"api_name\": \"name_of_api\",\n",
      "        #         \"parameters\": {param_dict}\n",
      "        #     }\n",
      "        # }\n",
      "        # For assistant responses:\n",
      "        # {\n",
      "        #     \"role\": \"assistant\",\n",
      "        #     \"text\": \"response text\"\n",
      "        # }\n",
      "```\n",
      "\n",
      "2. Integration with the Tool Executor:\n",
      "The `ToolExecutor` class handles the execution of tools and maintains the state. You would use it like this:\n",
      "\n",
      "```python\n",
      "# Initialize the tool executor\n",
      "executor = ToolExecutor(init_database_dir=\"path/to/your/databases\")\n",
      "\n",
      "# Create your predictor\n",
      "your_predictor = YourCustomPredictor(function_docs=your_function_docs)\n",
      "\n",
      "# Run a conversation\n",
      "results = executor.run_conversation(\n",
      "    conversation=your_conversation_data,\n",
      "    predict_func=your_predictor\n",
      ")\n",
      "```\n",
      "\n",
      "3. Key Components to Implement:\n",
      "\n",
      "a. Function Documentation:\n",
      "- Your predictor needs to understand the available tools\n",
      "- Each tool should be documented in a format that your interface can understand\n",
      "- The original implementation uses JSON format for tool definitions\n",
      "\n",
      "b. Conversation Format:\n",
      "Your conversation data should follow this structure:\n",
      "```python\n",
      "{\n",
      "    \"metadata\": {\n",
      "        \"timestamp\": \"YYYY-MM-DD HH:MM:SS\"\n",
      "    },\n",
      "    \"conversation\": [\n",
      "        {\n",
      "            \"role\": \"user\",\n",
      "            \"text\": \"user message\"\n",
      "        },\n",
      "        {\n",
      "            \"role\": \"assistant\",\n",
      "            \"text\": \"assistant response\",\n",
      "            \"apis\": [  # ground truth API calls\n",
      "                {\n",
      "                    \"request\": {\n",
      "                        \"api_name\": \"ToolName\",\n",
      "                        \"parameters\": {...}\n",
      "                    },\n",
      "                    \"response\": {...},\n",
      "                    \"exception\": null\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "4. Handling Tool Execution:\n",
      "- The `ToolExecutor` will handle the actual execution of tools\n",
      "- It maintains state across the conversation\n",
      "- It tracks things like session tokens and database states\n",
      "- It validates predictions against ground truth\n",
      "\n",
      "5. Evaluation:\n",
      "The system will automatically:\n",
      "- Track successful API calls\n",
      "- Calculate precision and recall\n",
      "- Identify incorrect actions\n",
      "- Provide metrics on the performance\n",
      "\n",
      "To adapt this to your interface:\n",
      "\n",
      "1. Implement your custom predictor\n",
      "2. Format your tool definitions to match the expected structure\n",
      "3. Format your conversation data according to the specified format\n",
      "4. Use the `ToolExecutor` to run and evaluate conversations\n",
      "\n",
      "The main adaptations would happen in:\n",
      "- Your predictor implementation\n",
      "- The format of your tool definitions\n",
      "- How you translate between your interface's format and ToolTalk's expected formats\n",
      "\n",
      "The core evaluation logic can remain unchanged as long as you conform to the expected input/output formats.\n",
      "\n",
      "Would you like me to show you a more detailed example of any of these components?\n"
     ]
    }
   ],
   "source": [
    "print(chunk['agent']['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Code Assistant Protocol**\n",
      "\n",
      "You are an advanced code editing assistant.\n",
      "\n",
      "**File Editing Protocol** (MUST FOLLOW when making changes):\n",
      "1. **Pre-Edit Verification** - ALWAYS use FileReaderTool to:\n",
      "   - Cross-validate paths using DirectoryMapperTool\n",
      "   - Get current file state WITH LINE NUMBERS\n",
      "   - Verify exact location for modifications\n",
      "   \n",
      "2. **Modification Rules**:\n",
      "   - Use FileEditorTool to make changes to the file.\n",
      "   - NEVER assume line numbers - ALWAYS verify via FileReaderTool\n",
      "   - NEVER include unchanged content - only send NEW/CHANGED lines\n",
      "   - Example: To add 1 line at position 42:\n",
      "     ✅ Correct: insert at line 42 with content\n",
      "     ❌ Wrong: Rewriting entire file with new line added\n",
      "\n",
      "**Token Efficiency Guidelines**:\n",
      "- Use precise parameters in ALL tool calls to avoid unnecessary tokens\n",
      "\n",
      "**General Operational Guidelines**:\n",
      "- Ask clarification questions if requirements are ambiguous\n",
      "# Task: Implement NVIDIA Evaluation Adaptor\n",
      "\n",
      "## Objective\n",
      "Create an adaptor that integrates the benchmark's core functionality with the `eval_core_utils` framework, enabling programmatic execution and evaluation.\n",
      "\n",
      "## Context\n",
      "- Repository Directory: /Users/khannguyen/projects/evaluations/ToolTalk\n",
      "- Instruction File: /Users/khannguyen/projects/agent-coding-template/use-case-tests/nvidia-eval-adaptors/nvidia_eval_adaptor_instruction.md\n",
      "\n",
      "## Required Actions\n",
      "\n",
      "1. Create Submodule Structure\n",
      "   - Name: `nvidia`\n",
      "   - Purpose: House all adaptor-related code\n",
      "\n",
      "2. Analyze Benchmark Entry Point\n",
      "   - Find how the benchmark is executed (usually a Python CLI script). The README.md file is a good place to start looking for entry point.\n",
      "   - Identify the main function/method that runs the benchmark\n",
      "\n",
      "3. Design Adaptor Interface\n",
      "   - Study `eval_core_utils` framework requirements\n",
      "   - Design interface that bridges benchmark logic with framework expectations\n",
      "   - Plan data model transformations\n",
      "\n",
      "4. Implementation Requirements\n",
      "   A. Core Integration\n",
      "      - Extract core benchmark logic from CLI implementation\n",
      "      - Create clean programmatic interface to this logic\n",
      "      - Implement data model transformations between benchmark and framework formats\n",
      "      \n",
      "   B. Adaptor Implementation\n",
      "      - Create adapter class(es) implementing required framework interfaces\n",
      "      - Transform framework inputs to benchmark format\n",
      "      - Execute benchmark logic directly\n",
      "      - Transform benchmark results to framework format\n",
      "      \n",
      "   C. Code Organization\n",
      "      - Keep all adaptor code within `nvidia` submodule\n",
      "      - Maintain clear separation between adaptation layer and core benchmark logic\n",
      "      - Use appropriate design patterns for clean integration\n",
      "\n",
      "## Key Guidelines\n",
      "1. NO Command-Line Invocation\n",
      "   - Do NOT shell out to CLI commands\n",
      "   - Do NOT use subprocess or similar approaches\n",
      "   - Access benchmark functionality directly at code level\n",
      "\n",
      "2. Clean Integration\n",
      "   - NO changes to existing benchmark code\n",
      "   - Use composition over inheritance where possible\n",
      "   - Maintain clear separation of concerns\n",
      "{'agent': {'messages': [AIMessage(content=[{'text': \"I'll help implement the NVIDIA Evaluation Adaptor following the requirements. Let me break this down into steps.\\n\\nFirst, let's explore the project structure to understand what we're working with.\", 'type': 'text'}, {'id': 'toolu_019BDPZKBEYUrqFXNKYLqHdK', 'input': {'directory': '/Users/khannguyen/projects/evaluations/ToolTalk', 'depth': 2}, 'name': 'DirectoryMapperTool', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_01FwM8QGdQBkKC9DhDkYVBaY', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 2179, 'output_tokens': 131}}, id='run-aa61d9b2-6f86-4bef-88b8-3cf000bc2e41-0', tool_calls=[{'name': 'DirectoryMapperTool', 'args': {'directory': '/Users/khannguyen/projects/evaluations/ToolTalk', 'depth': 2}, 'id': 'toolu_019BDPZKBEYUrqFXNKYLqHdK', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2179, 'output_tokens': 131, 'total_tokens': 2310, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='ToolTalk/\\n├── CODE_OF_CONDUCT.md\\n├── CONTRIBUTING.md\\n├── LICENSE\\n├── NOTICE\\n├── README.md\\n├── SECURITY.md\\n├── SUPPORT.md\\n├── data/\\n├── images/\\n├── output/\\n├── requirements.txt\\n├── scripts/\\n├── setup.cfg\\n├── setup.py\\n├── src/\\n└── tests/', name='DirectoryMapperTool', id='168b2104-6a95-41e2-88de-af05ecdb9ecf', tool_call_id='toolu_019BDPZKBEYUrqFXNKYLqHdK')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content=[{'text': \"Let's first create the necessary submodule structure under `src/`. Let me check what's currently in the src directory:\", 'type': 'text'}, {'id': 'toolu_01GnDMhhwSfwqP8u24VCXR6D', 'input': {'directory': '/Users/khannguyen/projects/evaluations/ToolTalk/src', 'depth': 2}, 'name': 'DirectoryMapperTool', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_01Kt8DNsvXaWGD7h2GTifm3u', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 2421, 'output_tokens': 119}}, id='run-d3a6f191-be88-46e2-8fcf-40f463c71066-0', tool_calls=[{'name': 'DirectoryMapperTool', 'args': {'directory': '/Users/khannguyen/projects/evaluations/ToolTalk/src', 'depth': 2}, 'id': 'toolu_01GnDMhhwSfwqP8u24VCXR6D', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2421, 'output_tokens': 119, 'total_tokens': 2540, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='src/\\n├── __init__.py\\n├── nvidia/\\n├── prompts/\\n├── scripts/\\n├── tooltalk/\\n└── tooltalk.egg-info/', name='DirectoryMapperTool', id='5febdf80-0628-46ac-a162-cc19e89be187', tool_call_id='toolu_01GnDMhhwSfwqP8u24VCXR6D')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content=[{'text': \"I see there's already a `nvidia` directory. Let's look at what's in it before proceeding:\", 'type': 'text'}, {'id': 'toolu_01UvcMXsda9vuntvqP9yjPRx', 'input': {'directory': '/Users/khannguyen/projects/evaluations/ToolTalk/src/nvidia', 'depth': 2}, 'name': 'DirectoryMapperTool', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_01LECzBikHypZwLrCWzrFTW1', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 2592, 'output_tokens': 118}}, id='run-cb8b11a9-7b2e-467c-b92b-7346f98ac3c9-0', tool_calls=[{'name': 'DirectoryMapperTool', 'args': {'directory': '/Users/khannguyen/projects/evaluations/ToolTalk/src/nvidia', 'depth': 2}, 'id': 'toolu_01UvcMXsda9vuntvqP9yjPRx', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2592, 'output_tokens': 118, 'total_tokens': 2710, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='nvidia/', name='DirectoryMapperTool', id='16d7b0e9-c95e-43c4-8c22-28ba5b80704d', tool_call_id='toolu_01UvcMXsda9vuntvqP9yjPRx')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content=[{'text': \"The `nvidia` directory exists but is empty. Let's also check the README to understand the benchmark's entry point:\", 'type': 'text'}, {'id': 'toolu_01PxbNyWMoq4xKm6m2TRpJmx', 'input': {'directory': '/Users/khannguyen/projects/evaluations/ToolTalk'}, 'name': 'ReadmeReaderTool', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_01EagTRapySp8NET6wXmi8HB', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 2723, 'output_tokens': 98}}, id='run-c41c9141-e332-4075-b70d-ddddedb27306-0', tool_calls=[{'name': 'ReadmeReaderTool', 'args': {'directory': '/Users/khannguyen/projects/evaluations/ToolTalk'}, 'id': 'toolu_01PxbNyWMoq4xKm6m2TRpJmx', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2723, 'output_tokens': 98, 'total_tokens': 2821, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='# :wrench: ToolTalk :speech_balloon:\\n\\n<p align=\"center\">\\n   <a href=\"https://arxiv.org/abs/2311.10775\" target=\"_blank\">:page_facing_up: Paper</a> | <a href=\"mailto:tooltalk@service.microsoft.com\" target=\"_blank\">:mailbox: Contact</a>\\n</p>\\n\\nIntroducing ToolTalk a benchmark for evaluating Tool LLMs in a conversational setting.\\n\\n## Details\\n\\n<div align=\"center\">\\n\\t<img src=\"images/Evaluation.png\" width=90% />\\n</div>\\n\\nToolTalk is designed to evaluate tool-augmented LLMs as a chatbot,\\nan increasingly popular paradigm for everyday users to harness the power of LLMs.\\nToolTalk contains a handcrafted dataset of 28 easy conversations and 50 hard conversations.\\nWe annotate these conversations to contain a ground truth usage of 28 unique tools belonging to 7 themed \"plugins\".\\n\\nEvaluation consists of prompting an LLM to predict the correct sequence of tools after every user utterance in a conversation.\\nThus, evaluating on a single conversation requires an LLM to correctly predict multiple sub-tasks.\\nPredictions are compared against the ground truth to determine success for a single conversation.\\n\\nWe evaluate two chatbots on ToolTalk powered by gpt-3.5-turbo-0613 and gpt-4-0613 implemented by using the chat completions API from OpenAI.\\n\\n| Model   | ToolTalk | Success rate | Precision | Recall | Incorrect Action Rate |\\n|---------|----------|--------------|-----------|--------|-----------------------|\\n| GPT-3.5 | Easy     | 85.7%        | 42.4%     | 89.3%  | 5.0%                  |\\n| GPT-4   | Easy     | 92.8%        | 69.2%     | 96.4%  | 3.8%                  |\\n| GPT-3.5 | Hard     | 26.0%        | 54.6%     | 69.7%  | 23.9%                 |\\n| GPT-4   | Hard     | 50.0%        | 74.9%     | 79.0%  | 25.1%                 |\\n\\n## Setup\\n\\nToolTalk can be setup using the following commands. Install local package with dev dependencies to enable unit tests.\\n\\n```bash\\npip install -r requirements.txt\\npip install -e \".[dev]\"\\n```\\n\\nTo verify that the installation was successful, run the unit tests.\\n\\n```bash\\npytest tests\\n```\\n\\n## Reproducing the results\\n\\nThe results on GPT-3.5-turbo and GPT-4 can be reproduced using the following commands. This requires having access to \\nOpenAI\\'s API. The results will be saved in the `results` folder. The script caches intermediary results, so it can be \\nre-run if it is interrupted for any reason.\\n\\n```bash\\nexport OPENAI_API_KEY=<your key>\\nbash evaluate_gpt35turbo.sh\\nbash evaluate_gpt4.sh\\n```\\n\\nYour results should look something like the number above, there will be some variance due to both models having non-deterministic results.\\n\\n## Generating scenarios\\n\\nTo generate new scenarios, you can use the following command.\\n\\n```bash\\npython -m tooltalk.generation.scenario_generator --prompt src/prompts/scenario_template.md --output_dir output/scenarios\\n```\\n\\n## Evaluating on new models\\n\\nThe easiest way to evaluate on new models would be to create a new `Predictor` class that inherits from `tooltalk.evaluation.tool_executor.BaseAPIPredictor`.\\nFor an example of how to do this, see `tooltalk.evaluation.tool_executor.GPT3Predictor` and `tooltalk.evaluation.evaluate_openai.OpenAIPredictor`.\\n\\n## Citing\\n\\n```\\n@article{farn2023tooltalk,\\n  title={ToolTalk: Evaluating Tool-Usage in a Conversation Setting},\\n  author={Nicholas Farn and Richard Shin},\\n  year={2023},\\n  journal={arXiv preprint arXiv:2311.10775},\\n}\\n```\\n\\n## Contributing\\n\\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\\n\\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\\nprovided by the bot. You will only need to do this once across all repos using our CLA.\\n\\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\\n\\n## Trademarks\\n\\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \\ntrademarks or logos is subject to and must follow \\n[Microsoft\\'s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\\nAny use of third-party trademarks or logos are subject to those third-party\\'s policies.\\n', name='ReadmeReaderTool', id='8f9678e4-e35e-4663-a450-5c063e8fa25b', tool_call_id='toolu_01PxbNyWMoq4xKm6m2TRpJmx')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content=[{'text': \"Now I'll create the required files for the NVIDIA adaptor. Based on the README, I can see that the main benchmark functionality is in the `tooltalk` package, particularly the evaluation components.\\n\\nLet's structure our NVIDIA adaptor module:\\n\\n1. First, let's create the basic structure in the `nvidia` directory:\", 'type': 'text'}, {'id': 'toolu_01JaQgMnCYhDz2znx4dR4QVy', 'input': {'directory': '/Users/khannguyen/projects/evaluations/ToolTalk/src/nvidia/evaluation'}, 'name': 'CreateDirectoryTool', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_01AeoZCBKh5sGJzbthffgxxV', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 4188, 'output_tokens': 149}}, id='run-de0630ea-4e32-41ec-ae61-6ba236958cf9-0', tool_calls=[{'name': 'CreateDirectoryTool', 'args': {'directory': '/Users/khannguyen/projects/evaluations/ToolTalk/src/nvidia/evaluation'}, 'id': 'toolu_01JaQgMnCYhDz2znx4dR4QVy', 'type': 'tool_call'}], usage_metadata={'input_tokens': 4188, 'output_tokens': 149, 'total_tokens': 4337, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content=\"Successfully created directory '/Users/khannguyen/projects/evaluations/ToolTalk/src/nvidia/evaluation'.\", name='CreateDirectoryTool', id='4a3c55f0-49df-4d11-b267-8625d465c893', tool_call_id='toolu_01JaQgMnCYhDz2znx4dR4QVy')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content=[{'text': \"Now let's create the necessary files:\\n\\n1. First, create the package __init__.py:\", 'type': 'text'}, {'id': 'toolu_01F5Z1gwLgQUbcrdFPCmjQVD', 'input': {'action': 'new', 'file_path': '/Users/khannguyen/projects/evaluations/ToolTalk/src/nvidia/__init__.py', 'content': '\"\"\"NVIDIA Evaluation Adaptor for ToolTalk benchmark.\"\"\"\\n\\nfrom nvidia.evaluation.adaptor import NVIDIAEvalAdaptor\\n\\n__all__ = [\\'NVIDIAEvalAdaptor\\']'}, 'name': 'FileEditorTool', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_01P1upqfZARh5aBGCJAzvxR6', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 4375, 'output_tokens': 188}}, id='run-beb0e3a3-3071-4aad-878a-30886295178a-0', tool_calls=[{'name': 'FileEditorTool', 'args': {'action': 'new', 'file_path': '/Users/khannguyen/projects/evaluations/ToolTalk/src/nvidia/__init__.py', 'content': '\"\"\"NVIDIA Evaluation Adaptor for ToolTalk benchmark.\"\"\"\\n\\nfrom nvidia.evaluation.adaptor import NVIDIAEvalAdaptor\\n\\n__all__ = [\\'NVIDIAEvalAdaptor\\']'}, 'id': 'toolu_01F5Z1gwLgQUbcrdFPCmjQVD', 'type': 'tool_call'}], usage_metadata={'input_tokens': 4375, 'output_tokens': 188, 'total_tokens': 4563, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content=\"Successfully created new file '/Users/khannguyen/projects/evaluations/ToolTalk/src/nvidia/__init__.py'.\", name='FileEditorTool', id='e04e8127-9f78-4a63-98aa-082dae334181', tool_call_id='toolu_01F5Z1gwLgQUbcrdFPCmjQVD')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content=[{'text': '2. Next, create the evaluation package __init__.py:', 'type': 'text'}, {'id': 'toolu_01TKDVQJNR2WXNaqCZzwWUwV', 'input': {'action': 'new', 'file_path': '/Users/khannguyen/projects/evaluations/ToolTalk/src/nvidia/evaluation/__init__.py', 'content': '\"\"\"NVIDIA Evaluation adaptor implementation.\"\"\"\\n\\nfrom nvidia.evaluation.adaptor import NVIDIAEvalAdaptor\\n\\n__all__ = [\\'NVIDIAEvalAdaptor\\']'}, 'name': 'FileEditorTool', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_01X1c3CoDEt3eQ8jZe3p62tK', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 4604, 'output_tokens': 176}}, id='run-dd796264-56d4-4451-99af-2aacfb60c689-0', tool_calls=[{'name': 'FileEditorTool', 'args': {'action': 'new', 'file_path': '/Users/khannguyen/projects/evaluations/ToolTalk/src/nvidia/evaluation/__init__.py', 'content': '\"\"\"NVIDIA Evaluation adaptor implementation.\"\"\"\\n\\nfrom nvidia.evaluation.adaptor import NVIDIAEvalAdaptor\\n\\n__all__ = [\\'NVIDIAEvalAdaptor\\']'}, 'id': 'toolu_01TKDVQJNR2WXNaqCZzwWUwV', 'type': 'tool_call'}], usage_metadata={'input_tokens': 4604, 'output_tokens': 176, 'total_tokens': 4780, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content=\"Successfully created new file '/Users/khannguyen/projects/evaluations/ToolTalk/src/nvidia/evaluation/__init__.py'.\", name='FileEditorTool', id='402475e6-2c21-4154-bd2a-8f302d14df1b', tool_call_id='toolu_01TKDVQJNR2WXNaqCZzwWUwV')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content=[{'text': \"3. Now let's create the main adaptor implementation:\", 'type': 'text'}, {'id': 'toolu_011LCVxKk7PzV3asw4C1i34T', 'input': {'action': 'new', 'file_path': '/Users/khannguyen/projects/evaluations/ToolTalk/src/nvidia/evaluation/adaptor.py', 'content': '\"\"\"NVIDIA Evaluation Adaptor implementation for ToolTalk benchmark.\"\"\"\\n\\nfrom typing import Any, Dict, List, Optional\\nfrom dataclasses import dataclass\\n\\nfrom tooltalk.evaluation.tool_executor import BaseAPIPredictor\\nfrom tooltalk.evaluation.types import Conversation, Prediction, ToolCall\\n\\n\\n@dataclass\\nclass NVIDIAEvalConfig:\\n    \"\"\"Configuration for NVIDIA evaluation adaptor.\"\"\"\\n    model_name: str\\n    model_kwargs: Optional[Dict[str, Any]] = None\\n    \\n\\nclass NVIDIAEvalAdaptor(BaseAPIPredictor):\\n    \"\"\"NVIDIA Evaluation Adaptor for ToolTalk benchmark.\\n    \\n    This adaptor integrates the ToolTalk benchmark with the eval_core_utils framework,\\n    enabling programmatic execution and evaluation of NVIDIA models.\\n    \"\"\"\\n    \\n    def __init__(self, config: NVIDIAEvalConfig):\\n        \"\"\"Initialize the NVIDIA evaluation adaptor.\\n        \\n        Args:\\n            config: Configuration for the evaluation adaptor.\\n        \"\"\"\\n        self.config = config\\n        self.model_kwargs = config.model_kwargs or {}\\n        # Initialize model here using config.model_name and model_kwargs\\n        \\n    def predict(self, conversation: Conversation) -> List[Prediction]:\\n        \"\"\"Predict tool calls for each user utterance in the conversation.\\n        \\n        Args:\\n            conversation: The conversation to generate predictions for.\\n            \\n        Returns:\\n            A list of predictions, one for each tool call in the conversation.\\n        \"\"\"\\n        predictions = []\\n        for turn in conversation.turns:\\n            # Transform conversation format to model input format\\n            model_input = self._prepare_model_input(turn, conversation)\\n            \\n            # Get model predictions\\n            raw_predictions = self._get_model_predictions(model_input)\\n            \\n            # Transform model output to ToolTalk prediction format\\n            prediction = self._convert_to_tooltalk_prediction(raw_predictions)\\n            predictions.append(prediction)\\n            \\n        return predictions\\n    \\n    def _prepare_model_input(self, turn: Any, conversation: Conversation) -> Dict[str, Any]:\\n        \"\"\"Prepare the model input from conversation turn.\\n        \\n        Args:\\n            turn: The current conversation turn.\\n            conversation: The full conversation context.\\n            \\n        Returns:\\n            Formatted input ready for model consumption.\\n        \"\"\"\\n        # Implement conversation format transformation\\n        raise NotImplementedError\\n        \\n    def _get_model_predictions(self, model_input: Dict[str, Any]) -> Any:\\n        \"\"\"Get predictions from the model.\\n        \\n        Args:\\n            model_input: Formatted input for the model.\\n            \\n        Returns:\\n            Raw model predictions.\\n        \"\"\"\\n        # Implement direct model invocation\\n        raise NotImplementedError\\n        \\n    def _convert_to_tooltalk_prediction(self, raw_predictions: Any) -> Prediction:\\n        \"\"\"Convert model output to ToolTalk prediction format.\\n        \\n        Args:\\n            raw_predictions: Raw output from the model.\\n            \\n        Returns:\\n            Formatted ToolTalk prediction.\\n        \"\"\"\\n        # Implement output format transformation\\n        raise NotImplementedError\\n        \\n    def _create_tool_call(self, tool_data: Dict[str, Any]) -> ToolCall:\\n        \"\"\"Create a ToolCall object from tool prediction data.\\n        \\n        Args:\\n            tool_data: Raw tool call data from model prediction.\\n            \\n        Returns:\\n            Formatted ToolCall object.\\n        \"\"\"\\n        # Implement tool call conversion\\n        raise NotImplementedError'}, 'name': 'FileEditorTool', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_012fuorLU5m3Wrz5u1nUgghJ', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 4823, 'output_tokens': 986}}, id='run-9ae87732-4dfb-4efe-9ef8-fb6769601854-0', tool_calls=[{'name': 'FileEditorTool', 'args': {'action': 'new', 'file_path': '/Users/khannguyen/projects/evaluations/ToolTalk/src/nvidia/evaluation/adaptor.py', 'content': '\"\"\"NVIDIA Evaluation Adaptor implementation for ToolTalk benchmark.\"\"\"\\n\\nfrom typing import Any, Dict, List, Optional\\nfrom dataclasses import dataclass\\n\\nfrom tooltalk.evaluation.tool_executor import BaseAPIPredictor\\nfrom tooltalk.evaluation.types import Conversation, Prediction, ToolCall\\n\\n\\n@dataclass\\nclass NVIDIAEvalConfig:\\n    \"\"\"Configuration for NVIDIA evaluation adaptor.\"\"\"\\n    model_name: str\\n    model_kwargs: Optional[Dict[str, Any]] = None\\n    \\n\\nclass NVIDIAEvalAdaptor(BaseAPIPredictor):\\n    \"\"\"NVIDIA Evaluation Adaptor for ToolTalk benchmark.\\n    \\n    This adaptor integrates the ToolTalk benchmark with the eval_core_utils framework,\\n    enabling programmatic execution and evaluation of NVIDIA models.\\n    \"\"\"\\n    \\n    def __init__(self, config: NVIDIAEvalConfig):\\n        \"\"\"Initialize the NVIDIA evaluation adaptor.\\n        \\n        Args:\\n            config: Configuration for the evaluation adaptor.\\n        \"\"\"\\n        self.config = config\\n        self.model_kwargs = config.model_kwargs or {}\\n        # Initialize model here using config.model_name and model_kwargs\\n        \\n    def predict(self, conversation: Conversation) -> List[Prediction]:\\n        \"\"\"Predict tool calls for each user utterance in the conversation.\\n        \\n        Args:\\n            conversation: The conversation to generate predictions for.\\n            \\n        Returns:\\n            A list of predictions, one for each tool call in the conversation.\\n        \"\"\"\\n        predictions = []\\n        for turn in conversation.turns:\\n            # Transform conversation format to model input format\\n            model_input = self._prepare_model_input(turn, conversation)\\n            \\n            # Get model predictions\\n            raw_predictions = self._get_model_predictions(model_input)\\n            \\n            # Transform model output to ToolTalk prediction format\\n            prediction = self._convert_to_tooltalk_prediction(raw_predictions)\\n            predictions.append(prediction)\\n            \\n        return predictions\\n    \\n    def _prepare_model_input(self, turn: Any, conversation: Conversation) -> Dict[str, Any]:\\n        \"\"\"Prepare the model input from conversation turn.\\n        \\n        Args:\\n            turn: The current conversation turn.\\n            conversation: The full conversation context.\\n            \\n        Returns:\\n            Formatted input ready for model consumption.\\n        \"\"\"\\n        # Implement conversation format transformation\\n        raise NotImplementedError\\n        \\n    def _get_model_predictions(self, model_input: Dict[str, Any]) -> Any:\\n        \"\"\"Get predictions from the model.\\n        \\n        Args:\\n            model_input: Formatted input for the model.\\n            \\n        Returns:\\n            Raw model predictions.\\n        \"\"\"\\n        # Implement direct model invocation\\n        raise NotImplementedError\\n        \\n    def _convert_to_tooltalk_prediction(self, raw_predictions: Any) -> Prediction:\\n        \"\"\"Convert model output to ToolTalk prediction format.\\n        \\n        Args:\\n            raw_predictions: Raw output from the model.\\n            \\n        Returns:\\n            Formatted ToolTalk prediction.\\n        \"\"\"\\n        # Implement output format transformation\\n        raise NotImplementedError\\n        \\n    def _create_tool_call(self, tool_data: Dict[str, Any]) -> ToolCall:\\n        \"\"\"Create a ToolCall object from tool prediction data.\\n        \\n        Args:\\n            tool_data: Raw tool call data from model prediction.\\n            \\n        Returns:\\n            Formatted ToolCall object.\\n        \"\"\"\\n        # Implement tool call conversion\\n        raise NotImplementedError'}, 'id': 'toolu_011LCVxKk7PzV3asw4C1i34T', 'type': 'tool_call'}], usage_metadata={'input_tokens': 4823, 'output_tokens': 986, 'total_tokens': 5809, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content=\"Successfully created new file '/Users/khannguyen/projects/evaluations/ToolTalk/src/nvidia/evaluation/adaptor.py'.\", name='FileEditorTool', id='372390ed-d205-4e37-9f9a-82fc18e1c60d', tool_call_id='toolu_011LCVxKk7PzV3asw4C1i34T')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content=\"The adaptor implementation follows these key design principles:\\n\\n1. **No CLI Invocation**: The adaptor directly interfaces with the model and benchmark code without using command-line tools or subprocesses.\\n\\n2. **Clean Integration**: \\n   - Inherits from `BaseAPIPredictor` to maintain compatibility with the benchmark\\n   - Uses composition through the configuration object\\n   - Maintains clear separation between adaptation layer and core benchmark logic\\n\\n3. **Core Integration**:\\n   - Implements the required `predict` method from BaseAPIPredictor\\n   - Provides clean transformation methods between formats\\n   - Structures code for easy maintenance and extension\\n\\n4. **Code Organization**:\\n   - All adaptor code is contained within the `nvidia` package\\n   - Clear separation of concerns through modular design\\n   - Configuration handled via dedicated dataclass\\n\\nThe implementation provides these key components:\\n\\n1. **NVIDIAEvalConfig**: Configuration dataclass for adaptor setup\\n2. **NVIDIAEvalAdaptor**: Main adaptor class implementing the benchmark interface\\n3. **Core Methods**:\\n   - `predict`: Main entry point for generating predictions\\n   - `_prepare_model_input`: Transforms conversation to model format\\n   - `_get_model_predictions`: Handles model invocation\\n   - `_convert_to_tooltalk_prediction`: Transforms model output to benchmark format\\n   - `_create_tool_call`: Helper for tool call conversion\\n\\nThe stub implementations will need to be completed with actual model integration code once the specific NVIDIA model interface is available. The structure is designed to make this integration straightforward while maintaining the clean architecture required by the specifications.\\n\\nTo complete the implementation, you'll need to:\\n\\n1. Integrate the specific NVIDIA model you want to use\\n2. Implement the transformation methods based on the model's input/output formats\\n3. Add any necessary model-specific configuration options to NVIDIAEvalConfig\\n\\nWould you like me to help with any of these next steps or would you like to share the specific NVIDIA model details for integration?\", additional_kwargs={}, response_metadata={'id': 'msg_01Vs449j6ABeWXiGwHuVNpxS', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 5853, 'output_tokens': 461}}, id='run-0c1674ec-b3b7-4f2e-aae0-5f7dd88a27db-0', usage_metadata={'input_tokens': 5853, 'output_tokens': 461, 'total_tokens': 6314, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, TypedDict, Annotated, Sequence\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from agent_coding_template.tools import *\n",
    "import uuid\n",
    "\n",
    "# Define state schema\n",
    "class AgentState(TypedDict):\n",
    "    messages: Sequence[HumanMessage]\n",
    "    thread_id: str\n",
    "\n",
    "# Create tools and model\n",
    "model = ChatAnthropic(model_name=\"claude-3-5-sonnet-20241022\", max_tokens=8000)\n",
    "tools = [DirectoryMapperTool(), FileReaderTool(), ReadmeReaderTool(), FileEditorTool(), CreateDirectoryTool(), FileCopyTool(), FileRenameTool()]\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "system_prompt = template = system_prompt_env.get_template(\"coding_agent_system_prompt.j2\").render()\n",
    "print(system_prompt)\n",
    "\n",
    "\n",
    "agent = create_react_agent(model, tools, checkpointer=memory, prompt=system_prompt)\n",
    "\n",
    "# Load the template file\n",
    "# template = usecase_prompt_env.get_template(\"1_add_dependencies.j2\")\n",
    "template = usecase_prompt_env.get_template(\"2_adaptor_bootstrap.j2\")\n",
    "\n",
    "content = template.render(repo_dir=\"/Users/khannguyen/projects/evaluations/ToolTalk\", adaptor_instruction_filepath=\"/Users/khannguyen/projects/agent-coding-template/use-case-tests/nvidia-eval-adaptors/nvidia_eval_adaptor_instruction.md\")\n",
    "print(content)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "\n",
    "# Stream results\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [HumanMessage(content=content)]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-coding-template",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
